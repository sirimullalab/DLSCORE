{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training workflow for DLScore version 3 <br>\n",
    "Changes: <br>\n",
    "<ul>\n",
    "    <li>With sensoring added. (But I don't think that helps, just to give it a try!) </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhassan/.conda/envs/tensorflow-py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras.utils import plot_model\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "import os.path\n",
    "import itertools\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import *\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import multiprocessing as mp\n",
    "from tqdm import *\n",
    "\n",
    "random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensoring outliers\n",
    "def sensoring(true, pred):\n",
    "    \"\"\" Sensor the predicted data to get rid of outliers\"\"\"\n",
    "    mn = np.min(true)\n",
    "    mx = np.max(true)\n",
    "    pred = np.minimum(pred, mx)\n",
    "    pred = np.maximum(pred, mn)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, pdb_ids, valid_size=0.1, test_size=0.1):\n",
    "    \"\"\"Converts the pandas dataframe into a matrix.\n",
    "    Splits the data into train, test and validations set.\n",
    "    Returns numpy arrays\"\"\"\n",
    "    # Load the indices of the non-zero columns.\n",
    "    # The same indices need to be used during the evaluation of test data\n",
    "    #with open(\"nonzero_column_indices.pickle\", \"rb\") as f:\n",
    "    #    non_zero_columns = pickle.load(f)\n",
    "    # Filter the zero columns out\n",
    "    #data = data[:, non_zero_columns]\n",
    "    \n",
    "    pdb_ids = np.array(pdb_ids)\n",
    "    \n",
    "    # Validation set\n",
    "    val_count = int(x.shape[0]*valid_size) # Number of examples to take\n",
    "    val_ids = np.random.choice(x.shape[0], val_count) # Select rows randomly\n",
    "    val_x = x[val_ids, :]\n",
    "    val_y = y[val_ids]\n",
    "    \n",
    "    # Save the pdb ids of the validation set in disk\n",
    "    with open('val_pdb_ids.pickle', 'wb') as f:\n",
    "        pickle.dump(pdb_ids[val_ids], f)\n",
    "    \n",
    "    # Remove validation set from data\n",
    "    mask = np.ones(x.shape[0], dtype=bool)\n",
    "    mask[val_ids] = False\n",
    "    x = x[mask, :]\n",
    "    y = y[mask]\n",
    "    pdb_ids = pdb_ids[mask]\n",
    "    \n",
    "    # Test set\n",
    "    test_count = int(x.shape[0]*test_size)\n",
    "    test_ids = np.random.choice(x.shape[0], test_count)\n",
    "    test_x = x[test_ids, :]\n",
    "    test_y = y[test_ids]\n",
    "    \n",
    "    # Save the pdb ids of the test set in disk\n",
    "    with open('test_pdb_ids.pickle', 'wb') as f:\n",
    "        pickle.dump(pdb_ids[test_ids], f)\n",
    "    \n",
    "    # Remove test set from data\n",
    "    mask = np.ones(x.shape[0], dtype=bool)\n",
    "    mask[test_ids] = False\n",
    "    x = x[mask, :]\n",
    "    y = y[mask]\n",
    "        \n",
    "    return x, y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y, pdb_ids, test_size=0.1):\n",
    "    \"\"\"Converts the pandas dataframe into a matrix.\n",
    "    Splits the data into train, test and validations set.\n",
    "    Returns numpy arrays\"\"\"\n",
    "    # Load the indices of the non-zero columns.\n",
    "    # The same indices need to be used during the evaluation of test data\n",
    "    #with open(\"nonzero_column_indices.pickle\", \"rb\") as f:\n",
    "    #    non_zero_columns = pickle.load(f)\n",
    "    # Filter the zero columns out\n",
    "    #data = data[:, non_zero_columns]\n",
    "    \n",
    "    pdb_ids = np.array(pdb_ids)\n",
    "    \n",
    "    # Test set\n",
    "    test_count = int(x.shape[0]*test_size)\n",
    "    test_ids = np.random.choice(x.shape[0], test_count)\n",
    "    test_x = x[test_ids, :]\n",
    "    test_y = y[test_ids]\n",
    "    \n",
    "    # Save the pdb ids of the test set in disk\n",
    "    with open('test_pdb_ids.pickle', 'wb') as f:\n",
    "        pickle.dump(pdb_ids[test_ids], f)\n",
    "    \n",
    "    # Remove test set from data\n",
    "    mask = np.ones(x.shape[0], dtype=bool)\n",
    "    mask[test_ids] = False\n",
    "    x = x[mask, :]\n",
    "    y = y[mask]\n",
    "        \n",
    "    return x, y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "def get_model(x_size, hidden_layers, dr_rate=0.5, l2_lr=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layers[0], activation=\"relu\", kernel_initializer='normal', input_shape=(x_size,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    for i in range(1, len(hidden_layers)):\n",
    "        model.add(Dense(hidden_layers[i],\n",
    "                        activation=\"relu\",\n",
    "                        kernel_initializer='normal',\n",
    "                        kernel_regularizer=regularizers.l2(l2_lr),\n",
    "                        bias_regularizer=regularizers.l2(l2_lr)))\n",
    "        model.add(Dropout(dr_rate))\n",
    "   \n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    return(model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_hidden_layers():\n",
    "#     x = [128, 256, 512, 768, 1024, 2048]\n",
    "#     hl = []\n",
    "    \n",
    "#     for i in range(1, len(x)):\n",
    "#         hl.extend([p for p in itertools.product(x, repeat=i+1)])\n",
    "    \n",
    "#     return hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(output_dir, serial=0):\n",
    "    if serial:\n",
    "        print('Running in parallel')\n",
    "    else:\n",
    "        print('Running standalone')\n",
    "    # Create the output directory\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    # Preprocess the data\n",
    "    pdb_ids = []\n",
    "    x = []\n",
    "    y = []\n",
    "    with open('Data_new.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader, None) # Skip the header\n",
    "        for row in reader:\n",
    "            pdb_ids.append(str(row[0]))\n",
    "            x.append([float(i) for i in row[1:349]])\n",
    "            y.append(float(row[349]))    \n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    # Normalize the data\n",
    "    mean = np.mean(x, axis=0)\n",
    "    std = np.std(x, axis=0) + 0.00001\n",
    "    x_n = (x - mean) / std\n",
    "    \n",
    "    # Write things down\n",
    "    transform  = {}\n",
    "    transform['std'] = std\n",
    "    transform['mean'] = mean\n",
    "    with open(output_dir + 'transform.pickle', 'wb') as f:\n",
    "        pickle.dump(transform, f)\n",
    "    \n",
    "    # Read the 'best' hidden layers\n",
    "    with open(\"best_hidden_layers.pickle\", \"rb\") as f:\n",
    "        hidden_layers = pickle.load(f)\n",
    "    \n",
    "    # Determine if running all alone or in parts (if in parts, assuming 8)\n",
    "    if serial:\n",
    "        chunk_size = (len(hidden_layers)//8) + 1\n",
    "        hidden_layers = [hidden_layers[i*chunk_size:i*chunk_size+chunk_size] for i in range(8)][serial-1]\n",
    "\n",
    "    # Network parameters\n",
    "    epochs = 100\n",
    "    batch_size = 128\n",
    "    keras_callbacks = [EarlyStopping(monitor='val_mean_squared_error',\n",
    "                                         min_delta = 0,\n",
    "                                         patience=20,\n",
    "                                         verbose=0)\n",
    "                       ]  \n",
    "\n",
    "    # Split the data into training and test set\n",
    "    train_x, train_y, test_x, test_y = train_test_split(x_n, y, pdb_ids, test_size=0.1)\n",
    "    #train_x, train_y, val_x, val_y, test_x, test_y = split_data(x_n, y, pdb_ids)\n",
    "    \n",
    "    pbar = tqdm_notebook(total=len(hidden_layers),\n",
    "                        desc='GPU: ' + str(serial))\n",
    "    for i in range(len(hidden_layers)):\n",
    "        if serial:\n",
    "            model_name = 'model_' + str(serial) + '_' + str(i)\n",
    "        else:\n",
    "            model_name = 'model_' + str(i)\n",
    "            \n",
    "        # Set dynamic memory allocation in a specific gpu\n",
    "        config = K.tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        if serial:\n",
    "            config.gpu_options.visible_device_list = str(serial-1)\n",
    "        K.set_session(K.tf.Session(config=config))\n",
    "        \n",
    "        # Build the model  \n",
    "        model = get_model(train_x.shape[1], hidden_layers=hidden_layers[i])\n",
    "        \n",
    "        # Save the model\n",
    "        with open(output_dir + model_name + \".json\", \"w\") as json_file:\n",
    "            json_file.write(model.to_json())\n",
    "        if not serial:\n",
    "            # If  not running with other instances then use 4 GPUs\n",
    "            model = multi_gpu_model(model, gpus=4)\n",
    "            \n",
    "        model.compile(\n",
    "            loss='mean_squared_error',\n",
    "            optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "            metrics=[metrics.mse])\n",
    "        #Save the initial weights\n",
    "        ini_weights = model.get_weights()\n",
    "        # 10 fold cross validation\n",
    "        kf = KFold(n_splits=10)\n",
    "        val_fold_score = 0.0\n",
    "        train_fold_score = 0.0\n",
    "        for _i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "            # Reset the weights\n",
    "            model.set_weights(ini_weights)\n",
    "            # Train the model\n",
    "            train_info = model.fit(train_x[train_index], train_y[train_index],\n",
    "                                   batch_size=batch_size,\n",
    "                                   epochs=epochs,\n",
    "                                   shuffle=True,\n",
    "                                   verbose=0,\n",
    "                                   validation_split=0.1,\n",
    "                                   #validation_data=(train_x[valid_index], train_y[valid_index]),\n",
    "                                   callbacks=keras_callbacks)\n",
    "            \n",
    "            current_val_predict = sensoring(train_y[valid_index], model.predict(train_x[valid_index])).flatten()\n",
    "            current_val_r2 = pearsonr(current_val_predict, train_y[valid_index])[0]\n",
    "            # If the current validation score is better then save it\n",
    "            if current_val_r2 > val_fold_score:\n",
    "                val_fold_score = current_val_r2\n",
    "                # Save the predicted values for both the training set\n",
    "                train_predict = sensoring(train_y[train_index], model.predict(train_x[train_index])).flatten()\n",
    "                train_fold_score = pearsonr(train_predict, train_y[train_index])[0]\n",
    "                # Save the training history\n",
    "                with open(output_dir + 'history_' + model_name + '_' + str(_i) + '.pickle', 'wb') as f:\n",
    "                    pickle.dump(train_info.history, f)\n",
    "        \n",
    "        # Save the results\n",
    "        dict_r = {} \n",
    "        dict_r['hidden_layers'] = hidden_layers[i]\n",
    "        dict_r['pearsonr_train'] = train_fold_score\n",
    "        dict_r['pearsonr_valid'] = val_fold_score\n",
    "        pred = sensoring(test_y, model.predict(test_x)).flatten()\n",
    "        dict_r['pearsonr_test'] = pearsonr(pred, test_y)[0]\n",
    "        #pred = sensoring(test_x, test_y, model.predict(test_x)).flatten()\n",
    "        # Write the result in a file\n",
    "        with open(output_dir + 'result_' + model_name + '.pickle', 'wb') as f:\n",
    "            pickle.dump(dict_r, f)\n",
    "        # Save the model weights\n",
    "        model.save_weights(output_dir + \"weights_\" + model_name + \".h5\")\n",
    "        # Clear the session and the model from the memory\n",
    "        del model\n",
    "        K.clear_session()\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'dl_networks_04/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [mp.Process(target=run, args=(output_dir, i)) for i in range(1, 9, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in parallel\n",
      "Running in parallel\n",
      "Running in parallel\n",
      "Running in parallel\n",
      "Running in parallel\n",
      "Running in parallel\n",
      "Running in parallel\n",
      "Running in parallel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149c6bf92fa94ce5a9a0205fc8ef375a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='GPU: 4', max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d7ad2fb69b4a8cab4b8539c2683382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='GPU: 3', max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363ae9354d1844fe97cdb38cbf5b4cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='GPU: 2', max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbd0cdc8f2b4ddaaa8055ac886a80a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='GPU: 6', max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c41b54c431648b5a09339c88df71a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='GPU: 5', max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3197b4574fe448bb912e034e74ff57a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='GPU: 1', max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f976a880ce5146e7b190cd49c7bde7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='GPU: 7', max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d96cc2247d543079cdd44f20e05eeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='GPU: 8', max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhassan/.conda/envs/tensorflow-py35/lib/python3.5/site-packages/scipy/stats/stats.py:3021: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  r = r_num / r_den\n",
      "/home/mhassan/.conda/envs/tensorflow-py35/lib/python3.5/site-packages/scipy/stats/stats.py:3021: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  r = r_num / r_den\n"
     ]
    }
   ],
   "source": [
    "for j in jobs:\n",
    "    j.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the network number and pearson coffs. of train, test and validation set in a list (in order)\n",
    "\n",
    "model_files = sorted(glob.glob(output_dir + 'model_*'))\n",
    "weight_files = sorted(glob.glob(output_dir + 'weights_*'))\n",
    "result_files = sorted(glob.glob(output_dir + 'result_*'))\n",
    "\n",
    "models = []\n",
    "r2 = []\n",
    "hidden_layers = []\n",
    "weights = []\n",
    "# net_layers = []\n",
    "for mod, res, w in zip(model_files, result_files, weight_files):\n",
    "    models.append(mod)\n",
    "    weights.append(w)\n",
    "    with open(res, 'rb') as f:\n",
    "        r = pickle.load(f)\n",
    "    coeff = [r['pearsonr_train'], r['pearsonr_test'], r['pearsonr_valid']]\n",
    "    r2.append(coeff)\n",
    "    hidden_layers.append(r['hidden_layers'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the indices according to the validation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_ar = np.array(r2)\n",
    "sorted_indices = list((-r2_ar)[:, 2].argsort())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_r2 = [r2[i] for i in sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9278997, 0.71250445, 0.7681534],\n",
       " [0.93809354, 0.6875171, 0.76786435],\n",
       " [0.9335826, 0.69356865, 0.76711303],\n",
       " [0.9088995, 0.6989541, 0.7666596],\n",
       " [0.94130564, 0.69059765, 0.76646286]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_r2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_models = [models[i] for i in sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dl_networks_04/model_4_1.json',\n",
       " 'dl_networks_04/model_5_3.json',\n",
       " 'dl_networks_04/model_2_5.json',\n",
       " 'dl_networks_04/model_2_3.json',\n",
       " 'dl_networks_04/model_1_6.json']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_models[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_weights = [weights[i] for i in sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dl_networks_04/weights_model_4_1.h5',\n",
       " 'dl_networks_04/weights_model_5_3.h5',\n",
       " 'dl_networks_04/weights_model_2_5.h5',\n",
       " 'dl_networks_04/weights_model_2_3.h5',\n",
       " 'dl_networks_04/weights_model_1_6.h5']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_weights[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the lists in the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir + 'sorted_models.pickle', 'wb') as f:\n",
    "    pickle.dump(sorted_models, f)\n",
    "with open(output_dir + 'sorted_r2.pickle', 'wb') as f:\n",
    "    pickle.dump(sorted_r2, f)\n",
    "with open(output_dir + 'sorted_weights.pickle', 'wb') as f:\n",
    "    pickle.dump(sorted_weights, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
